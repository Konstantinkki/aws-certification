S3 Lifecycle Rules (with S3 Analytics)

Transition Objects between storage Classes 
Standard
  Standard IA
    Intelligent Tiering
      One-Zone IA
        Glacier Instant Retrival
	  Glacier Flexible Retrival 
	    Glacier Deep Archive

--------------------------------------------------------------------------------
Lifecycle rules

Moving objects can be automated using a Lifecycle Rules

  Transition Actions - configure objects to transition to another storage class
	Example : Move objects to Standard IA class 60 days after creation

  Expiration Actions - configure objects to expire (delete) after some time
	Examples : Access log files can be set to delete after a 365 days
		   Can be used to delete old versions of files (if versioning is enabled)
		   Can be used to delete incoming Multi-Part uploads
  Rules can be created for a certain prefix (example: S3://mybucket/mp3/*)
  Rules can be created for certain objects Tags(example: Department: Finance)

Scenario 1
	Your application on EC2 created images zthumbnails after profile photos
	are uploaded to Amazon S3, These thumbnails can be easily recreated, 
	and only need to be keept for 60 deys, The source images should be able 
	to be immediately retrived for these 60 days, and afterwards, the user
	can wait up to 6 hours, How would you design this ?
  Solution : 
	S3 source images can be on Standard, with a lifecycle configuration to 
	transition them to Glacier after 60 days
	
	S3 thumbnails can be on One-Zone IA, with a lifecycle configuration to 
	expire them (delete them) agter 60 days

Scenario 2
	A rule in your company states that you should be able to recover your 
	deleted S3 objects immediately for 30 days, although this may happen
	rerely. After this time, and for up to 365 days, deleted objects should be 
	recoverable within 48 hours
  Solution : 
	Enable S3 Versioning in order to have object versions, so that "delete Objects"
	are in fact hidden by a "delete marker" and can be recovered

	Transition the "noncurrent versions" of the object to Standard IA

	Transition afterwards the "noncurrent versions" to Glacier Deep Archive

---------------------------------------------------------------------------------
Amazon S3 Analytics - Storage Class Analysis

	Help you decide when to transition objects to the right storage class

	Recomendations for Standard and Standard IA
	  Does NOT work for One-Zone IA or Glacier

	Report is updated daily
	
	24 - 48 hours to start seeing data analysis

	Good first step to put together Lifecycle Rules (or ,improve them)!

------------
Create configuration : 
	Amazon S3 -> Buckets -> storage-clesses-demo -> Lifecycle configuration

============================================================================================
S3 Event Notifications
	S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...

	Object name filtering possible (*.jpg)

	UseCase: generate thumbnails of images uploaded to S3

		generated event may be sent to:  SNS, SQS, Lambda

	Can create as many "S3 events" as desired

	S3 event notifications typically deliver events in seconds but can sometimes take 
	a minute or longer

--------------------------------------------------------------------------------------------
S3 Event Notifications - IAM Permissions

	for SNS - SNS Resource (Access) Policy
	for SQS - SQS Resource (Access) Policy
	for Lambda - Lambda Resource Pilicy

---------------------------------------------------------------------------------------------
S3 Event Notifications with Amazon EventBridge

	S3 event -> Amazon EventBridge -> Over 18 AWS services as destinations

	Advanced filtering options with json rules (metadata, object size, name ...)
	Multiple Destinations - ex Step Function, Kinesis Streams / Firehouse...
	EventBridge Capabilities - Archive, Replay Events, Reliable delivery
==============================================================================================
S3 Baseline Performance
	Amazon S3 automatically scales to high requests rates, latency 100-200 ms
	Your application can achieve at least 3500 PUT/COPY/POST/DELETE or
	5500 GET/HEAD requests per second per 'prefix' in a bucket.
	There are no limitsto the number of prefixes in a bucket.
	
	Example (object path => prefix):
	  bucket/folder1/sub1/file => /folder1/sub1/
	  bucket/folder1/sub2/file => /folder1/sub2/
	  bucket/1/file		   => /1/
	  bucket/2/file 	   => /2/
	
	if you spred reads across all four prefixes evenly, you can achive 22000 
	requests per second for GET and HEAD

----------------------------------------------------------------------------------------------------
S3 Performance
	Multi-Part upload :
		recomended for files > 100MB
		MUST use for files > 5Gb
		Deviding to parts can help parallelize uploads (speed up transfer)
---------------------------------------------------------------------------------------------------
S3 Performance - S3 Byte-Range Fetches
	Parallelize GETs by requesting specific byte ranges
	Better resilience in case of failures

Can be used to speed up doenloads

Can be used to retrive only partial data (for example the head of a file)  (first XX bytes)

===================================================================================================
S3 Select & Glacier Select 
	Retrive less data using SQL by performing server-side filtering
	Can filter by rows & columns (Simple SQL statements)
	Less network transfer, less CPU cost client-side
==================================================================================================
S3 User-defined Object Metadata & S3 Object Tags
	S3 User-defined tObject Metadata
		When uploading an object, you can also assign metadata
		Name-value (key-value) pairs
		User-defined metadata names must begin with "x-amz-meta-"
		Amazon S3 stores user-defined metadata keys in lowercase
		Metadata can be retrived while retriving the object

	S3 Object Tags
		Key-value pairs for objects in Amazon S3
		Useful for fine-grained permissions (only access specific objects with specific tags)
		Useful for analitics purposes (using S3 Analytics to group by tags)

	You cannot search the object netadata or object tags
	instead, you must use an external DB as a search index such as DynamoDB
	
	
















	



