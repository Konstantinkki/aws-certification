AWS Integration and Messaging
--------------------------------------
Section Introduction
	When we start deploying multiple applications, they will inenvitably need to communicate with other another
	There are two patterns of application communication

	1. Synchronous communications (application to application)
	2. Asynchronous / Event Based (application to queue to application)

	Synchronous between applications can be problematic if there are sudden spikes of traffic
	What if you need to suddenly encode 1000 videos but usually it's 10 ?

	In that case, it's better to decouple your applications,
		using SQS: queue model
		using SNS: pub/sub model
		using Kunesis: real-time streaming model
	These services can scale independently from our application
=================================================================================================================
Amazon SQS Standard Queues Overview
1..n Producers ---send messages---> SQS Queue --->poll messages by--->1..n consumers
-----------------------------------------------------------------------------------------------------------------
Amazon SQS - Standard Queue
	Oldest offering (over 10 years)
	Fully managed service, used to decouple applications
	
	Attributes:
		Unlimited throughput, unlimited number of messages in queue
		Default retention of messages: 4 days, maximum of 14 days
		Low latency (<10ms on publish and receive)
		Limitation of 256Kb per message sent

	Can have duplicate messages (at least once delivery, occasionally)
	Can have out of order messages (best effort ordering)
-----------------------------------------------------------------------------------------------------------------
SQS - Producing Messages
	Produced to SQS using the SDK (Send Messages API)
	The message is persisted in SQS untill a consumer deletes it 
	Message retention: default 4 days, up to 14 days

	Example: send an order to be processed
		Order Id
		Customer Id
		Any attributes you want

	SQS standard: unlimited throughput
-----------------------------------------------------------------------------------------------------------------
SQS Consuming Messages
	Consumers (running on EC2 instances, servers, or AWS Lambda)...
	Poll SQS for messages (receive up to 10 messages at a time)
	Process the messages (example: insert the message into an RDS database)
	Delete the messages using the DeleteMessage API

SQS Multiple EC2 Instances Consumers
	Consumers receive and process messages in parallel
	At least once delivery
	Best-effort message ordering
	Consumers delete messages after processing them
	We can scale consumers horizontally to improve throughput of processing 
------------------------------------------------------------------------------------------------------------------
SQS with ASG
	Common integration
	SQS Queue -------------------------------------->ASG [EC2 Instances]
	   |                                              ^
	   |                                              |
	   |                                             scale command
	  \/                                              |
    cloudWatch metric - Queue Length  ---alarm-->  CloudWatch Alarm
	(ApproximateNumberOfMessages)
------------------------------------------------------------------------------------------------------------------
SQS to decouple between application tiers



user requests ------> ASG[Front-end web app] --send message--> SQS Queue ---receive--> ASG[Back-end processing Application] ---insert---> S3 Bucket
-------------------------------------------------------------------------------------------------------------------
Amazon SQS - Security
	Encryption: 
		In-flight encryption using HTTPS API
		At-rest encryption using KMS keys
		Client-side encryption if the client wants to perform encryption/decryption itself

	Access Controls : IAM policies to regulate access to the SQS API
	SQS Access Policies (similar to S3 bucket policies)
		Useful for cross-account access to SQS queues
		Useful for allowing other services (SNS, S3 ...) to write to an SQS queue
=========================================================================================================================
SQS Queue Access Policy
	Cross account access 

account 444455556666                                   account 111122223333
  SQS Queue            ---poll for messages------>         EC2 Instance

{
"Version": "2012-10-17",
"Statement":[{
	"Effect": "Allow",
	"Principal":{"AWS":["1111222233334444"]},
	"Action": ["sqs:ReceiveMessage"],
	"Resource": "arn:aws:sqs:us-east-1:444455556666:queue1"
}]
}

- - - - - 
Publish S3 Event Notifications to SQS Queue
Upload object ---> S3 Bucket (bucket1) ----send message----> SQS Queue queue1

{
"Version": "2012-10-10",
"Statement": [{
        "Effect":"Allow",
	"Principal":{"AWS":"*"},
	"Action":["sqs:SendMessage"],
	"Resource":"arn:aws:sqs:us-east-1:444455556666:queue1",
     	"Condition":{
		"ArnLike":{"aws:SourceArn":"arn:aws:s3:*.*:bucket1"},
		"StringEquals": {"aws:SourceAccount":"<bucket1_owner_account_id>"}
	}
    }]
}


==============================================================================================
SQS - Message Visibility Timeout
	After a message is polled by a consumer, it becomes invisible to other consumers
	By default, the "message visibility timeout" is 30 seconds
	That means the message has 30 seconds to be processed
	After the message visibility timeout is over, the message is "visible" in SQS
	If a message is not processed within the visibility timeout, it will be processed twice
	A consumer could call the ChangeMessageVisibility API to get more time
	If visibility timeout is high (hours), and consumer crashes, re-processing  will take time
	If visibility timeout is too low (seconds), we may get duplicates
----------------------------------------------------------------------------------------------
Amazon SQS - Dead Letter Queue (DLQ)
	if a consumer fails to process a message within the Visibility Timeout... the message goes back to the queue!
	We can set a threshold of how many times a message can go back to the queue
	After the MaximumReceives threshold is exceeded, the message goes into a dead leter queue (DLQ)
	Useful for debuging!
	DLQ of a FIFO queue must also be a FIFO queue
	DLQ of a Standard queue must also be a standard queue
	Make sure to process the messages in the DLQ before they expire:
		Good to set a retention of 14 days in the DLQ
------------------------------------------------------------------------------------------------
SQS DLQ - Redrive to Source
	Feature to help consume messages in the DLQ to understand what is wrong with them
        When our code is fixed, we can redrive the messages from the DLQ back into the source
		queue (or any other queue) in batches without writing custom code 
=================================================================================================
Amazon SQS - Delay Queue
	Deley a message (consumers don't see it immediately) up to 15 minutes
	Default is 0 seconds (message is available right away)
	Can set a default at queue level
	Can override the default on send using the "DelaySeconds" parameter  - queue will send message to COnsumer after specified count of seconds
=================================================================================================
Amazon SQS Long Polling
	When a consumer requests messages from the queue, it can optionally "wait" for messages to arrive if there are none in the queue
	This is called Long Polling
	LongPolling decresses the number of API calls madeto SQS while increasing the afficiency and latency of your application
	The wait time can be between 1 sec to 20 sec (20 sec preferable)
	Long Polling is preferable to Short Polling
	Long polling can be enabled at the queue level or at the API level using ReceiveMessageWaitTimeSeconds
-----------------------------------------------------------------------------------------------------
SQS Extended Client
	Message size limit is 256Kb, how to send large messages, e.g. 1Gb?
	Using the SQS Extended client (java Library)

	Solution:
		producer send small message with metadata into SQS Queue, but huge file will put into s3 Bucket
		consumer gets message with metadata and decide ult to huge data into S3
			getting it and processing
---------------------------------------------------------------------------------------------------------
SQS - Must know API
	CreateQueue (MessageRetentionPeriod), Delete Queue
	PurgeQueue: delete all the messages in queue
	SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage
	MaxNumberOfMessages: default 1 max 10 (for ReceiveMessage API)
	ReceiveMessageWaitTimeSeconds: Long Polling
	ChangeMessageVisibility: change the message timeout

	Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your costs
==========================================================================================================
Amazon SQS - FIFO Queue
	FIFO = First In First Out (ordering of messages in the queue)
	Limited throughput : 300 msg/s without batching , 3000 msg/s with
	Exactly-once send capability (by removing duplicates)
	Messages are processed in order by the consumer
===========================================================================================================
SQS FIFO - Deduplicate
	De-duplication interval is 5 minutes
	Two de-duplication methods:
		Content baseddeduplication : will do a SHA-256 hash of the message body
		Explicity provide a Message Deduplication ID
------------------------------------------------------------------------------------------------------------
SQS FIFO - Message Grouping
	if you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order
	To get ordering at the level of a subset of messages, specify different values for MessageGroupId
		Messages that share a common Message Group ID will be in order within the group 
		Each Group ID can have a different consumer (parallel processing)
		Ordering across groups is not guaranteed
=============================================================================================================
Amazon SNS
	The "event producer" only sends messages to one SNS topic
	As many "events" (sucscriptions) as we want to listen to the SNS topic notifications
	Each subscriber to the topic will get all the messages (note: new feature to filter messages)
	Up to 12 500 000 subscribers per topic
	100 000 topics limit

	SNS ---puslish---> SQS, Lambda, Kinesis Data Firehouse, Emails, SMS & Mobile Notifications, HTTP(S) Endpoints
-------------------------------------------------------------------------------------------------------------------------
SNS integrates with a lot of AWS services
	Many AWS services can send data dirrectly to SNS for notifications	

	CloudWatdh, AWS Budgets, Lambda,
	ASG, S3, DynamoDB,                                               -------publish-------> SNS
	CloudFormation (State Changed), AWS DMS(New Replic), RDS Events 

-------------------------------------------------------------------------------------------------------------------------
AWS SNS - How to publish
	Topic Publish (using the SDK)
	Create a topic
	Create a subscription (or many)
	Publish to the topic

	Direct Publish (for mobile apps SDK)
		Create a platform application
		Create a platform endpoint
		Publish to the platform endpoint
		Works with Google GCM, Apple APNS, Amazon ADM ...
---------------------------------------------------------------------------------------------------------------------------
Amazon SNS - Security
	Encryption: 
		In-flight encryption using HTTPS API
		At-rest encryption using KMS keys
		Client-side encryption if the client wants to perform encryption/decryption itself

	Access Controls: IAM policies to regulate access to the SNS API
	SNS Access Policies (similar to S3 bucket policies)
		Useful for cross-account access to SNS topics
		Useful for allowing other services (S3...) to write to an SNS topic
--------------------------------------------------------------------------------------------------------------------------
SNS + SQS: Fan Out
	push once in SNS, receive in all SQS queues that are subscribers
	Fylly decoupled, no data loss
	SQS allows for : data persistence, delayed processing and retries of work
	Ability to add more SQS subscribers over time
	Make sure your SQS queue access policy allows for SNS to write
	Cross-Region Delivery: works with SQS Queues  in other regions
-------------------------------------------------------------------------------------------------------------------------
Application : S3 Events to multiple queues
	For the combination of : event type (e.g. Object create) and prefix (e.g. images/) you can only have one S3 event rule
	If you want to send the same S3 event to many SQS queues, use fan-out

 S3 --send event--> SNS Topic --broadcast to ---> 2*queues + Lambda

-------------------------------------------------------------------------------------------------------------------------
Application : SNS to Amazon S3 through Kinesis Data Firehouse
	SNS can send to Kinesis and therefore we canhave the following solutions architecture:

buying service --send message--> SNS Topic ----> Kinesis Data Firehouse --broadcast to---> S3, Any supported KDF Destination
--------------------------------------------------------------------------------------------------------------------------
Amazon SNS - FIFO Topic
	Similar features as SQS FIFO:
		Ordering by Message Group ID (All messages in the same group are ordered)
		Deduplication using a Deduplication ID or Content Based Deduplication
	Can only have SQS FIFO queues as subscribers
	Limited throughput ( same throughput as SQS FIFO )
--------------------------------------------------------------------------------------------------------------------------
SNS FIFO + SQS FIFO : Fan-out
	In case you need fan-out + ordering + deduplication
--------------------------------------------------------------------------------------------------------------------------
SNS - Message Filtering
	JSON policy used to filter messages sent to SNS topic's subscriptions
	If a subscription doesn't have a filter policy, it receives every message
	


	                                          Filter Policy(State: Palced) -->SQS Queue(Placed order)
buying Service ---Order:1036  -->SNS Topic -->    Filter Policy(Canceled Orders) --> SQS Queue (Canceled Orders)
		  Product: pencil                 Filter Policy(Canceled Orders) --> Email Subscription (Canceled Orders)
		  Qty: 4                          Filter Policy(State Declined) --> SQS Queue (Declined Orders)
		  State: Placed                   No Filters --> SQS Queue (All)
=========================================================================================================================
SNS Example : 
	goto SNS 
	create topic
	create subscription Email
	accept in mail box
	Send message into topic 
	check message into email
=========================================================================================================================
Kinesisi Overview
	Makes it easy to collect, process, and analyze streaming data in real-time
	Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data ...

	"Kinesis DataStreams": capture, process, and store data streams
	"Kinesis Data Firehose": load data streams into AWS data store 
	"Kinesis Data Analytics": analize data streams with SQL or Apache Flink
	"Kinesis Video Streams": capture, process, and store video streams
-------------------------------------------------------------------------------------------------------------------------
Kinesis Data Stream
	producers   -----------------send record --------------> to one of Shards (1..N) into KDS ----send record-----------------> Consumers
	Applications                 [Partition Key]                                                  [Partition Key]                  Apps (K.Client.L. SDK)
	Clients (PC, phone)          [Data Blob (up to 1 Mb)]                                         [Sequence no.]                  LAmbda
	SDK, Kin Producer Lib        1Mb/sec or 1000msg/sec                                           [Data Blob]                     Kinesis Data Firehose
	Kinesis Agent	                 per 1 shard                                                  2Mb/sec per                     Kinasis Data Analitics
												      shard all consumers
												               OR
												      2Mb/sec (enhanced)
												     per shard per consumer
-------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Data Streams
	Retention between 1 day to 365 days
	Ability reprocess (replay) data
	Once data is inserted in Kinesis, it can't be deleted (immutability)
	Data that shares the same partition goes to the same shared (ordering)
	Producers : AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
	
	Consumers:
		Write your own: Kinesis Client Library (KCL), AWS SDK
		Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics
--------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Data Streams - Capacity Modes
	Provisioned Mode :
		You choose the Number of shards provisioned, scale manually or using API
		Each shard gets 1Mb/s INPUT (or 1000 records per second)
		Each shardmgets 2Mb/s OUTPUT (classic or enhanced fan-out consumer)
		You pay per shard provisioned per hour

	On-demand mode:
		No need to provision or manage the capacity
		Default capacity provisioned (4Mb/s INPUT or 4000 records per second)
		Scales automatically based on observed throughput peak during the last 30 days
		Pay per stream per hour & data in/out per GB
---------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Data Stream Security
	Control access / authorization using IAM policies
	Encryption in flight using HTTPS endpoints
	Encryption at rest using KMS
	You can implement encryption/decryption of data on client side (harder)
	VPC Endpoints available for Kinesis to access within VPC
	Monitor API calls using CLoudTrial	
--------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Producers
	Puts data records into data stream
	Data record consists of :
		Sequence number (unique per partition-key within shard)
		Partition Key (must specify while put records into stream)
		Data blob (up to 1 Mb)
	Producers:
		AWS SDK : simple producer
		Kinesis Producer Library (KPL) : C++, Java,  - batch, compression, retries
		Kinesis Agent: monitor log files
	Write throughput: 1Mb/sec or 1000 records/sec per shard
	PutRecord API
	Use batching with PutRecords API to reduce cost & increase throughput
---------------------------------------------------------------------------------------------------------------------------------------------
Kinesis - ProvisionedThroughputExceeded
	Solution :
		Use highly distributed partition key
		Retries with exponential backoff
		Increase shards (scaling)
=============================================================================================================================================

Kinesisi Data Streams Consumers
	Get data records from data streams and process them

	AWS Lambda
	Kinesisi Data Analytics
	Kinesis Data Firehose
	Custom Consumer (AWS SDK) - Classic or Enhanced Fan-Out
	Kinesis Client Library (KCL) : library to simplify reading from data stream
---------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Consumers Types
	Shared (classic) Fan-out Consumer - pull
		Low number of consuming applications
		Read throughput: 2 Mb/sec per shard across all consumers
		Max 5 GetRecords API calls/sec
		Latency ~200 ms
		Minimize cost ($)
		Consumers poll data from Kinesis using GetRecords API call
		Returns up to 10 MB (then throttle for 5 seconds ) or up to 10000 records

	Enhanced Fan-out consumer - push
		Multiple consuming applications for the same stream
		2Mb/sec per consumer per shard
		Latency ~70ms
		Higher qcosts ($$$)
		Kinesis pushes data to consumers over HTTP/2 (SubscribeToShared API)
		Soft limit of 5 consumer applications (KCL) per data stream (default)
----------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Consumers - AWS Lambda
	Supports classoc and enhanced fan-out consumers
	Read records in batches
	Can configure batch size and batch window
	If error occurs, Lambda retries util succeeds or data expired
	Can process up to 10 batches per shard simultaneously

==============================================================================================================================================
Kinesis Ckient Library (KCL)
	A java Library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload
	Each shard *is to be read by only one KCL instance !!!
	Progress is checkpointed into DynamoDB (needs IAM access)
	Track other workers and share the work amongst shards using DynamoDB
	KCL can run on EC2, Elastic Beanstalk and on premises
	Records are read in order at the shard level
	Versions: 
		KCL 1.x - supports shared consumer
		KCL 2.x - supports fan-out consumer

	Example :
		we have Kinesis Data Stream with 6 shards
		for rean we should run 6 instances with KCL
		Instances will checkpoint progress into DynamoDB 
		If some instance is fail down  another instance will know progress of failed instance    z
==============================================================================================================================================
Kinesis Operation - Shard Splitting
	Used to increase the Stream capacity (1Mb/s data in per shard)
	Used to devide a "hot shard"
	The Old shard is closed and will be detected once the data is expired 
	No automatic scaling (manually increase/decrease capacity)
	Can't split into more than two shards in a single operation
----------------------------------------------------------------------------------------------------------------------------------------------
Kinesis Operation - Merging Shards 
	Decrease the Stream capacity and save costs
	Can be used to group two shards with low traffic (could shards) 
	Old shards are closed and will be deleted once the data is expired
	Can't merge more than two shards in a single operation
==============================================================================================================================================
Kinesis Data Firehose
	producers will push data into firehose, and firehose will push data to consumers

	Producers:
		Aoolications
		Clients (PC, phone)
		SDK, KPL
		Kinesisi Agent
		Kinesis Data Streams
		Amazon CloudWatch (Log, events)
		AWS IoT

	Up to 1 Mb record 
		Firehose may transform data with Lambda Function
		Firehose may push Dailed data into S3

	Consumers: 
		Amazon S3
		Amazon Redshift (copy through S3)
		Amazon OpenSearch

		Custom Http Endpoint

		3-rd party Partners
			Datadog
			Sputnik
			New Relic
			MongoDB

	Pay for data through Firehose
	Near real time
		60 seconds latency minimum for non full batches
		Or minimum 1 MB of data at a time
	Supports many data formats, conversions, transformations, compression
	Supports custom data transformation using Lambda
	Can send failed or all data to a backup S3 Bucket
-------------------------------------------------------------------------------------------------------
Kinesis Data Stream vs Firehose
	Kinesis Data Stream :
		Streaming Service for ingest at scale
		Write custom code (producer/consumer)
		Real time (~200ms)
		Manage scaling (shard splitting/merging)
		Data storage for 1 to 365 days
		Supports replay capability

	Kinesis Data Firehode: 
		Load streaming data into S3/Redshift/OpenSearch/3-rd Party/ custom HTTP
		Fully managed
		Near real-time (buffer time min 60 sec)
		Automatic Scaling
		No data storage
		Doesn't support replay capability
======================================================================================================
Kinasis Data Analytics for SQL applications
	Real-time analytics on Kinesis Data Stream & Firehose using SQL
	Add reference data from Amazon S3 to enrich streaming data 
	Fully managed, no servers to provision
	Automatic scaling
	Pay for actual consumption rate

	Output:
		Kinesis Data Stream: create streams out of the real-time analytics queries
		Kinesis Data Firehose: send analytics query result to destinations
	Use cases:
		Time-series analytics
		Real-time dashboards
		Real-time metrics
--------------------------------------------------------------------------------------------------------
Kinesis Data Analytics for Apache Flink  (Amazon Managed Service for Apache Flink)
	Use Flink (Java, Scala, SQL) to process and analyze streaming data
	
	Example : 
		Kinesis Data Stream
		                    -----------> Kinesis Data Analytics For Apache Flink
		Amazon MSK

	Run any Apache Flink application on a managed cluster on AWS
		provisioning compute resources, parallel computation, automatic scaling
		application backups (implemented as checkpoints and snapshots)
		Use any Apache Flink programming features
		Flink does not read from Firehose (use Kinesis Analytics for SQL instead)
===========================================================================================================
Ordering Data into Kinesis
	You have 100 trucks on the road sending their GPS positions regularly into AWS
	You want to consume the data in order for each truck, so that you can track their movement accurately.
	How should you send that data into Kinesis?

	Answer: send using a "Partition Key" value of the "truck_id"
	The same key will always go to the same shard
------------------------------------------------------------------------------------------------------------
Ordering Data into SQS
	For SQS standard, there is no ordering.
	For SQS FIFO, if you don't use a Group ID, messages are consumed in the order they are sent, with only one consumer
	You want to scale the number of consumers, but you want messages to be "grouped"  when they are related to each other
	Then you use a Group ID (similar to Partition Key in Kinesis)
------------------------------------------------------------------------------------------------------------
Kinesis vs SQS ordering
	Let's assume 100 trucks, 5 kinesis shards, I SQS FIFO
	
	Kinesis Data Streams:
		On average you'll have 20 trucks per shard
		Trucks will have their data ordered within each shard
		The maximum amount of consumers in parallel we can have is 5
		Can receive up to 5 MB/s of data
	
	SQS FIFO
		You only have one SQS FIFO queue
		You will have 100 Group ID
		you can have up to 100 Consumers (due to the 100 Group ID)
		you have up to 300 messages per second (or 3000 if using batching)
============================================================================================================
SQS vs SNS vs Kinesis

SQS    
	Consumer "pull data"
	Data is deleted after being consumed
	Can have as many workers (consumers) as we want
	No need to provision throughput
	Ordering guaranties only on FIFO queues
	Individual message delay capability
                  
SNS 
	Push data to many subscribers
	Up to 12 500 000 subscribers
	Data is not persisted (lost if not delivered)
	Pub/Sub
	Up to 100 000 topics
	No need to provision throughput
	Integrates with SQS for fan-out architecture pattern
	FIFO capability for SQS FIFO

Kinesis
	Standard : pull data 
		2 MB per shard
	Enhanced-fan out: push data
		2 MB per shared per consumer
	Possibility to replay data
	Meant for real time big data, analytics and ETL
	Ordering at the shard level
	Data expires afrer X days
	Provisioned mode or on-demand capacity mode

	
	
	

		
		

	






			
















	




	
 








